### 字符串编码

作者：Windson Yang
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

我们在学习编程的过程一定会遇到unicode, UTF-8, ASCII, Latin-1这些编码术语和问题，那么什么是编码，我们又会遇到什么问题呢？OK，花上15分钟来学习一下吧。

### 基本术语

#### 字符（character）

    A B C 天 气 エ ン コ

上面的用空格分开的都是字符，也就是对人类有意义的文字。当然，如果不懂中文或者日文的话会觉得没有意义，但是这不代表这对其他懂的人无意义

#### 字符串（strings）

    Hello 天气

### 字符集

顾名思义，多个字符组成的集合就是字符集

#### 编码

把字符串按照一定的模式（既可以是某些规则，或者根据对应表）转换成二进制数字

#### 解码

与编码相反，把二进制数字按照一定的模式转换成字符串然后显示或者保存

### 计算机如何存储数据
无论是图片，数字，文字还是各种数据，计算机都是以数字“0”或者“1”存储的。也因为计算机最初由西方国家设计以及发展，所以他们就使用了自己的语言作为常用的字符集，以英文为例，字符集包括大小写字母，数字加上一些标点符号，运算符号大概100个左右。因为早期的计算机存储资源非常宝贵（请自行搜索下3.5英寸软盘）。计算机科学家希望用最少的空间来存储字符，二进制用3位就能表示8种不同的字符：

    000 001 010 011 100 101 110 111
      A   B   C   D   E   F   G   H

也就是说计算机在硬盘读取数据的时候如果它看到的是

    001100100

会先以3位为单位分割，然后根据我们一开始的对应表，转换成相应的字母，至于为什么000表示的A，001代表的是B，其实都是可以自己设计的，你只需要告诉计算机这个对应关系，计算机就会知道如何转换。

    001 100 100
      B   E   E

不过3位明显不够存储这100多个字符，2的7次方128已经足够了，加多一位用作检查错误，不过计算机更喜欢处理8位为基数的数字（我们现在的计算机一般都是64位或者32位的）所以最后选择使用8位来存储字符，刚好一个字节。8位数字对应的表就更长了：

![ascii-table]()


现在计算机存储“Cherry”这个字符串，就会存储下面的“0”和“1”在硬盘内，读取的时候再对照上面的表显示出来就好。

    1000011 1101000 1100101 1110010 1110010 1111001
          C       h       e       r       r       y


### 字符串编码

上面的表把

    C -> 1000011

这样的过程称为编码（encode），也就是说寻找一种模式（某些规则，或者对应表）来把字符串用二进制数字来显示并且保存。解码（decode）则相反，把二进制数字转换为字符串的过程。早期计算机科学家统一用这张表作编码，称为[ASCII编码](https://en.wikipedia.org/wiki/ASCII)。现在问题一来了：

![unicode-problem]()

ASCII编码不够用，例如中文，常用字就几千个，其他亚洲语言也遇到这样的问题。所以以前国内流行的并不是ASCII编码，而是GBK编码。本质其实也一样，用两个字节，一张更大的表，用更多的位来表示字符。GBK的编码方式比较有趣，是可变长度的编码，一个小于127的字符的意义与原来相同，但两个大于127的字符连在一起时，就表示一个汉字。为了兼容ASCII编码，GBK使用了单字节编码和双字节编码。也就是说当我们把“Cherry”用GBK编码存储的时候，它依然会存储为

    1000011 1101000 1100101 1110010 1110010 1111001
          C       h       e       r       r       y

*如果只存储ASCII编码出现的字符，那么解码的时候使用什么编码都不会出错*

只有当要存储一些没有出现在ASCII表中的字符，例如“Cherry 你好”，GBK编码会存储为

    1000011 1101000 1100101 1110010 1110010 1111001 100000 11000100 11100011 10111010 11000011
          C       h       e       r       r       y   空格                你                好


这时候“你”和“好”都是用两个字节保存，问题来了，如果使用GBK解码的时候，如果计算机读到ASCII出现的二进制数字，例如上面的“C”，“h”...就会用ASCII来进行解码，当遇到ASCII表中没有的二进制数字，例如当他遇到“y”后面的第一个二进制数字

    11000100

它发现在ACSCII表中招不到对应的字符（11000100代表的是十进制188），它就知道这个是两个字节组成的GBK表专有的字符，于是它把

    11000100 11100011

找到GBK编码对应的“你”然后显示出来。不过这样就会遇到我们常见的编码问题，如果一个文件用GBK编码存储起来，然后用ASCII解码，就会出错。也就是说当你用GBK编码保存了文件，然后你的朋友使用ASCII编码打开就会报错。因为ASCII表中并没有对应的字符串显示。下面以python3为例


    >>> hello = "Cherry 你好" # 定义字符串
    >>> hello_gbk = hello.encode('gb2312') # 用GBK编码把字符串转换成二进制
    >>> hello_gbk 
    # 这里是上面1的16进制表示
    # b'Cherry \xc4\xe3\xba\xc3' = 1000011 1101000 1100101 1110010 1110010 1111001 100000 11000100 11100011 10111010 11000011
    b'Cherry \xc4\xe3\xba\xc3'
    >>> hello_gbk.decode('ascii') # 如果用ASCII解码则报错，因为计算机在ASCII表中找不到对应的显示字符
    Traceback (most recent call last):
      File "<stdin>", line 1, in <module>
    UnicodeDecodeError: 'ascii' codec can't decode byte 0xc4 in position 7: ordinal not in range(128)

这个问题其实也容易解决，只需要在文档的信息上面添加这是用什么编码的，打开的时候选择对应的解码就好，不过另外一个问题就比较头痛，很多国家都开始使用自己的编码，例如日本使用Shift JIS，韩国使用KS X 1001。而且即使是中文，还有繁体中文，简体中文，编码经过发展也有几个不同版本。这样下去就越来越乱了，即使你知道文档里面存的是中文，也不知道用什么编码才能正确打开，有时候文档中没有包含编码信息的时候，计算机可能就会猜这是什么编码，举例

    >>> hello = "你好"
    >>> hello_gbk = hello.encode("gb2312")
    >>> hello_gbk
    b'\xc4\xe3\xba\xc3'
    >>> hello_gbk.decode("Shift-JIS")
    'ﾄ羲ﾃ'
    >>> hello_gbk.decode("utf-8")
    Traceback (most recent call last):
      File "<stdin>", line 1, in <module>
    UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc4 in position 0: invalid continuation byte

在上面的例子“你好”经过GBK编码编程四个字节，之后用日本的Shift JIS版本居然也能显示出来，这个情况下虽然没有报错，但是显然不是我们想要的内容。只是Shift JIS中刚好有这四个字节对应的字符串而已，如果我们用另外一种编码UTF-8来解码，就会因为招不到对应的字符串而报错。

### Unicode

如果你足够聪明的话，最简单解决的方法就是，用足够长的二进制位来存储世界上所有字符串。这并不是痴人说梦，Unicode就是为此而生，其实很简单，每个国家的每个字符都编进去，最新的版本已经有 136,755个字符串了。例如你好，对应的是 4F60 597D。不同于之前的其他编码，Unicode只是规定了表现形式，至于如何存储则可以使用不同的编码，这个什么意思呢，例如“你好”这个字符串，如果直接使用四个字节保存的话，那么就是UTF-32编码，UTF-32编码是定长编码。很直观，既然用四个字节表达所有字符串了，那么保存的话直接用四个字节保存就好了。这没有任何技术上的问题，而且也运行得不错吧。

### UTF-8

UTF-8编码其实只是一种hack，因为UTF-32编码把所有字符都用四个字节保存，基本只用英文的人原本使用ASCII编码只需要一个字节，加上以前计算机的存储空间很贵，网络传输速度非常慢，平白无故增加三倍显然不乐意。于是计算机科学家开发了另外一个可变长度的编码UTF-8来存储Unicode编码。具体怎么实现呢？其实很简单，首先我们参考ASCII表，使用它所有的内容，当计算机读取到ASCII中有的内容，则按照ASCII表进行转换，然后像GBK编码一样按顺序，把一些常用的字符用两个字节来保存。少用的就保存为三个字节。所以即使你用UTF-8编码保存英文，用ASCII也能正确解码

    >>> hello = "hello"
    >>> hello.encode('utf-8')
    b'hello'
    >>> b'hello'.decode('ascii')
    'hello'

这样即使旧的电脑也能正确地解码英文


### UTF-8与Unicode有什么关系？
Unicode虽然也是由数字组成，不过它把常用的字符也用了4个字节来保存，UTF-8是一种可变长度编码，把常用的保存为1个字节


| character     | encoding  | bits              |
| ------------- |:---------:| -----------------:|
| A             | UTF-8     |          01000001 |
| A             | UTF-16    | 00000000 01000001 |
| A             | UTF-32    | 00000000 00000000 00000000 01000001 |



### 实际使用遇到的问题
![test_unicode]()
>>> py2_strings = "Hi, 我是Cherry"

要了解计算机编码必须了解计算机如何存储字符串

>>> strings = "Hi, 我是Cherry"

U+6211 U+662F


字符串编码的讨论

我们把字符串编码分成几大类：
    unicode编码
    utf类编码（utf-8, utf-16编码）
    其它编码（包括askii编码，gbk编码,latin编码)
